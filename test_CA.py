#!/usr/bin/env python
# coding: utf-8

# # Application for the data-dependent covering algorithms on real data

# In[1]:

import numpy as np
import pandas as pd


from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from six import StringIO
from sklearn.tree import export_graphviz
import pydotplus

import rulefit
import CoveringAlgorithm.CA as CA
import CoveringAlgorithm.functions as func
import CoveringAlgorithm.covering_tools as ct

# import matplotlib.pyplot as plt

target_dict = {'student': 'G3',
               'boston': 'MEDV',
               'bike': 'cnt',
               'wine': 'quality'}

racine_path = r'/home/vincent/Dropbox/Th√®se/Papers/EJS/Codes/Data/'


def load_data(name):
    if name == 'student':
        data = pd.read_csv(racine_path + 'Student/student-por.csv',
                           sep=';')
        data['sex'] = [1 if x == 'F' else 0 for x in data['sex'].values]
        data['Pstatus'] = [1 if x == 'A' else 0 for x in data['Pstatus'].values]
        data['famsize'] = [1 if x == 'GT3' else 0 for x in data['famsize'].values]
        data['address'] = [1 if x == 'U' else 0 for x in data['address'].values]
        data['school'] = [1 if x == 'GP' else 0 for x in data['school'].values]
        data = data.replace('yes', 1)
        data = data.replace('no', 0)
    elif name == 'bike':
        data = pd.read_csv(racine_path + 'BikeSharing/hour.csv', index_col=0)
        data = data.set_index('dteday')
    elif name == 'boston':
        from sklearn.datasets import load_boston
        boston_dataset = load_boston()
        data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
        data['MEDV'] = boston_dataset.target
    elif name == 'wine':
        data = pd.read_csv(racine_path + 'Wine/winequality-white.csv', sep=';')
    else:
        raise ValueError('Not tested dataset')

    target = target_dict[data_name]
    y = data[target]
    X = data.drop([target], axis=1)

    return X, y


if __name__ == '__main__':
    #  Data parameters
    data_name = 'bike'  # 'boston' 'bike' 'student' 'wine'

    normalize = False
    seed = 42
    np.random.seed(seed)
    test_size = 0.3

    # RF parameters
    tree_size = 4  # number of leaves
    max_rules = 2000  # total number of rules generated by RF
    nb_estimator = int(np.ceil(max_rules / tree_size))  # Number of tree in the forest
    max_depth = 100  # useless

    # AdBoost and GradientBoosting
    learning_rate = 0.01

    # Covering parameters
    alpha = 1. / 2 - 1 / 100.
    gamma = 0.95
    generator_func = AdaBoostRegressor

    # ## Data Generation
    X, y = load_data(data_name)
    if normalize:
        y -= y.mean()
        y /= y.std()

    features = X.describe().columns  # To get only numerical data
    X = X[features].values

    # ### Splitting data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,
                                                        random_state=seed)
    if test_size == 0.0:
        X_test = X_train
        y_test = y_train

    # Normalization of the error
    deno_aae = np.mean(np.abs(y_test - np.median(y_test)))
    deno_mse = np.mean((y_test - np.mean(y_test)) ** 2)

    subsample = min(0.5, (100 + 6 * np.sqrt(len(y_test))) / len(y_test))

    # ## Decision Tree
    tree = DecisionTreeRegressor(max_leaf_nodes=tree_size,
                                 random_state=seed,
                                 max_depth=max_depth)
    tree.fit(X_train, y_train)

    tree_rules = ct.extract_rules_from_tree(tree, features, X_train.min(axis=0),
                                            X_train.max(axis=0))

    # dot_data = StringIO()
    # export_graphviz(tree, out_file=dot_data,
    #                 filled=True, rounded=True,
    #                 special_characters=True)
    # graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
    # graph.create_png()

    # ## Random Forests generation
    regr_rf = RandomForestRegressor(n_estimators=nb_estimator,
                                    max_leaf_nodes=tree_size,
                                    random_state=seed,
                                    max_depth=max_depth)
    regr_rf.fit(X_train, y_train)

    # importances = regr_rf.feature_importances_
    # std = np.std([tree.feature_importances_ for tree in regr_rf.estimators_],
    #              axis=0)
    # indices = np.argsort(importances)[::-1]
    #
    # print("Feature ranking:")
    # for f in range(X.shape[1]):
    #     print("%d. %s (%f)" % (f + 1, features[indices[f]], importances[indices[f]]))

    # # Plot the feature importances of the forest
    # plt.figure()
    # plt.title("Feature importances")
    # plt.bar(range(X.shape[1]), importances[indices],
    #         color="r", yerr=std[indices], align="center")
    # plt.xticks(range(len(features)), [features[i] for i in indices])
    # plt.xlim([-1, X.shape[1]])
    # plt.show()

    rf_rule_list = []
    for tree in regr_rf.estimators_:
        rf_rule_list += ct.extract_rules_from_tree(tree, features, X_train.min(axis=0),
                                                   X_train.max(axis=0))

    # ## GradientBoosting
    gb = GradientBoostingRegressor(n_estimators=nb_estimator,
                                   max_leaf_nodes=tree_size,
                                   learning_rate=learning_rate,
                                   subsample=subsample,
                                   random_state=seed,
                                   max_depth=max_depth)
    gb.fit(X_train, y_train)
    gb_rule_list = []
    for tree in gb.estimators_:
        gb_rule_list += ct.extract_rules_from_tree(tree[0], features, X_train.min(axis=0),
                                                   X_train.max(axis=0))

    # ## AdBoost
    ad = AdaBoostRegressor(n_estimators=nb_estimator,
                           learning_rate=learning_rate,
                           random_state=seed)
    ad.fit(X_train, y_train)
    ad_rule_list = []
    for tree in ad.estimators_:
        ad_rule_list += ct.extract_rules_from_tree(tree, features, X_train.min(axis=0),
                                                   X_train.max(axis=0))

    # ## Covering Algorithm RandomForest
    ca_rf = CA.CA(alpha=alpha, gamma=gamma,
                  tree_size=tree_size,
                  seed=seed,
                  max_rules=max_rules,
                  generator_func=RandomForestRegressor)
    ca_rf.fit(x=X_train, y=y_train, features=features)

    print('Covering Algorithm RF selected set of rules covering:', ca_rf.selected_rs.calc_coverage())
    # print('Covering Algorithm selected set of rules:', ca.selected_rs.to_df())

    # a = func.plot_counter_variables(ca.selected_rs)
    # b = func.plot_dist(ca.selected_rs)

    # ## Covering Algorithm GradientBoosting
    ca_gb = CA.CA(alpha=alpha, gamma=gamma,
                  tree_size=tree_size,
                  seed=seed,
                  max_rules=max_rules,
                  generator_func=GradientBoostingRegressor)
    ca_gb.fit(x=X_train, y=y_train, features=features)

    print('Covering Algorithm GB selected set of rules covering:', ca_gb.selected_rs.calc_coverage())
    # print('Covering Algorithm selected set of rules:', ca.selected_rs.to_df())

    # a = func.plot_counter_variables(ca.selected_rs)
    # b = func.plot_dist(ca.selected_rs)

    # ## Covering Algorithm
    ca_ad = CA.CA(alpha=alpha, gamma=gamma,
                  tree_size=tree_size,
                  seed=seed,
                  max_rules=max_rules,
                  generator_func=AdaBoostRegressor)
    ca_ad.fit(x=X_train, y=y_train, features=features)

    print('Covering Algorithm AD selected set of rules covering:', ca_ad.selected_rs.calc_coverage())
    # print('Covering Algorithm selected set of rules:', ca.selected_rs.to_df())

    # a = func.plot_counter_variables(ca.selected_rs)
    # b = func.plot_dist(ca.selected_rs)

    # ## RuleFit
    rule_fit = rulefit.RuleFit(tree_size=tree_size,
                               max_rules=max_rules,
                               random_state=seed,
                               max_iter=2000)
    rule_fit.fit(X_train, y_train)

    # ### RuleFit rules part
    rules = rule_fit.get_rules()
    rules = rules[rules.coef != 0].sort_values(by="support")
    rules = rules.loc[rules['type'] == 'rule']

    # ### RuleFit linear part
    lin = rule_fit.get_rules()
    lin = lin[lin.coef != 0].sort_values(by="support")
    lin = lin.loc[lin['type'] == 'linear']

    rulefit_rules = ct.extract_rules_rulefit(rules, features, X_train.min(axis=0),
                                             X_train.max(axis=0))

    # ## Errors calculation
    pred_tree = tree.predict(X_test)
    pred_rf = regr_rf.predict(X_test)
    pred_gb = gb.predict(X_test)
    pred_ad = ad.predict(X_test)
    pred_CA_rf = ca_rf.predict(y_train, X_test)
    pred_CA_gb = ca_gb.predict(y_train, X_test)
    pred_CA_ad = ca_ad.predict(y_train, X_test)
    pred_rulefit = rule_fit.predict(X_test)

    print('Bad prediction for Covering Algorithm RF:',
          sum([x == 0 for x in pred_CA_rf]) / len(y_test))
    print('Bad prediction for Covering Algorithm GB:',
          sum([x == 0 for x in pred_CA_gb]) / len(y_test))
    print('Bad prediction for Covering Algorithm AD:',
          sum([x == 0 for x in pred_CA_ad]) / len(y_test))
    # ## Results.
    print('Interpretability score')
    print('Decision tree interpretability score:', func.inter(tree_rules))
    print('Random Forest interpretability score:', func.inter(rf_rule_list))
    print('Gradient Boosting interpretability score:', func.inter(gb_rule_list))
    print('AdBoost interpretability score:', func.inter(ad_rule_list))
    print('Covering Algorithm RF interpretability score:', func.inter(ca_rf.selected_rs))
    print('Covering Algorithm GB interpretability score:', func.inter(ca_gb.selected_rs))
    print('Covering Algorithm AB interpretability score:', func.inter(ca_ad.selected_rs))
    print('RuleFit interpretability score:', func.inter(rulefit_rules))
    print('Linear relation:', len(lin))

    print('')
    print('aae')
    print('Decision Tree aae:', np.mean(np.abs(y_test - pred_tree)) / deno_aae)
    print('Random Forest aae:', np.mean(np.abs(y_test - pred_rf)) / deno_aae)
    print('Gradient Boosting aae:', np.mean(np.abs(y_test - pred_gb)) / deno_aae)
    print('AdaBoost aae:', np.mean(np.abs(y_test - pred_ad)) / deno_aae)
    print('Covering Algorithm RF aae:', np.mean(np.abs(y_test - pred_CA_rf)) / deno_aae)
    print('Covering Algorithm GB aae:', np.mean(np.abs(y_test - pred_CA_gb)) / deno_aae)
    print('Covering Algorithm AD aae:', np.mean(np.abs(y_test - pred_CA_ad)) / deno_aae)
    print('RuleFit aae:', np.mean(np.abs(y_test - pred_rulefit)) / deno_aae)
    print('')
    print('MSE')
    print('Decision Tree mse:', np.mean((y_test - pred_tree) ** 2) / deno_mse)
    print('Random Forest mse:', np.mean((y_test - pred_rf) ** 2) / deno_mse)
    print('Gradient Boosting mse:', np.mean((y_test - pred_gb) ** 2) / deno_mse)
    print('AdaBoost mse:', np.mean((y_test - pred_ad) ** 2) / deno_mse)
    print('Covering Algorithm RF mse:', np.mean((y_test - pred_CA_rf) ** 2) / deno_mse)
    print('Covering Algorithm GB mse:', np.mean((y_test - pred_CA_gb) ** 2) / deno_mse)
    print('Covering Algorithm AD mse:', np.mean((y_test - pred_CA_ad) ** 2) / deno_mse)
    print('RuleFit mse:', np.mean((y_test - pred_rulefit) ** 2) / deno_mse)
    print('')
    print('R2 score')  # Percentage of the explained variance
    print('Decision Tree R2 score', r2_score(y_test, pred_tree))
    print('Random Forest R2 score', r2_score(y_test, pred_rf))
    print('Gradient Boosting R2 score', r2_score(y_test, pred_gb))
    print('AdaBoost R2 score', r2_score(y_test, pred_ad))
    print('Covering Algorithm RF R2 score', r2_score(y_test, pred_CA_rf))
    print('Covering Algorithm GB R2 score', r2_score(y_test, pred_CA_gb))
    print('Covering Algorithm AD R2 score', r2_score(y_test, pred_CA_ad))
    print('RuleFit R2 score', r2_score(y_test, pred_rulefit))
